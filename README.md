# What Is This?

A simple docker-compose setup to run local LLMs with Ollama. CPU and AMDGPU configs are available.

Uncomment Open WebUI to run it.

# Models I Recommend

These are okay models capable running on higher-end desktop hardware.

- [llama3.1:8b](https://ollama.com/library/llama3.1)
- [llama3.2:3b](https://ollama.com/library/llama3.2)
- [qwen2.5-coder:7b](https://ollama.com/library/qwen2.5-coder)
- [qwen2.5-coder:14b](https://ollama.com/library/qwen2.5-coder)
- [deepseek-coder-v2:16b](https://ollama.com/library/deepseek-coder-v2)
