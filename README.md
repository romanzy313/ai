A simple docker-compose setup for local llm with Ollama and Open WebUI.
